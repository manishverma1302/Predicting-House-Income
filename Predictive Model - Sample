import pandas
import webbrowser
import os

# Read the dataset into a data table using Pandas
data_table = pandas.read_csv("C:/Users/Manish Verma/Desktop/acs_ny.csv")

# Create a web page view of the data for easy viewing
html = data_table[0:100].to_html()

# Save the html to a temporary file
with open("data.html", "w") as f:
    f.write(html)

# Open the web page in our web browser
full_filename = os.path.abspath("data.html")
webbrowser.open("file://{}".format(full_filename))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import ensemble
from sklearn.metrics import mean_absolute_error
from sklearn.externals import joblib

# Load the data set
df = pd.read_csv("C:/Users/Manish Verma/Desktop/acs_ny.csv")

# Remove the fields from the data set that we don't want to include in our model
del df['Acres']
del df['YearBuilt']
del df['FoodStamp']

# Replace categorical data with one-hot encoded data
features_df = pd.get_dummies(df, columns=['FamilyType', 'NumUnits', 'OwnRent', 'HeatingFuel', 'Language'])

# Remove the family income from the feature data because this is going to be our output
del features_df['FamilyIncome']


# Create the X and y arrays
# X is the standard name for input arrays and y is the standard name for expected output to predict
# as.matrix because to make sure it as numpy matrix data type not the pandas dataframe
X = features_df.as_matrix()
y = df['FamilyIncome'].as_matrix()

# Split the data set in a training set (70%) and a test set (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Fit regression model
model = ensemble.GradientBoostingRegressor(
    n_estimators=1000,   # How many decision trees to be build
    learning_rate=0.1,   # Learning rate controls how much each additional decision tree influences the overall prediction. 
                         # Lower rates usually lead to higher accuracy but only works 
                         # if we have n_estimators set to a high value.
    max_depth=6,         # Max_depth controls how many layers deep each individual decision tree can be 
    min_samples_leaf=9,  # Min_samples_leaf controls how many times a value must appear in our training set 
                         # for a decision tree to make a decision based on it.
                         # Let's set it to 9. We are saying that at least 9 houses must exhibit the same characteristic 
                         # before we consider it meaningful enough to build a decision tree around it.
                         #This helps prevent single outliers from influencing the model too much. 
    
    max_features=0.1,   # Max_features is the percentage of features in our model that we randomly choose to consider 
                        # each time we create a branch in our decision tree.
    loss='huber',       #The huber function does a good job while not being too influenced by outliers in the data set.
    random_state=0
)
model.fit(X_train, y_train) # we tell the model to train using our training data set by calling scikit-learn's fit 
                            #function on the model. This is just a single line of code.

# Save the trained model to a file so we can use it in other programs
joblib.dump(model, 'acs_ny_familyincome_classifier_model.pkl')

# Find the error rate on the training set
mse = mean_absolute_error(y_train, model.predict(X_train))
print("Training Set Mean Absolute Error: %.4f" % mse)

# Find the error rate on the test set
mse = mean_absolute_error(y_test, model.predict(X_test))
print("Test Set Mean Absolute Error: %.4f" % mse)

import numpy as np
from sklearn.externals import joblib

# These are the feature labels from our data set
feature_labels = np.array(['NumBedrooms','NumChildren','NumPeople','NumRooms','NumVehicles','NumWorkers','HouseCosts','ElectricBill','Insurance','FamilyType_Female Head', 'FamilyType_Male Head', 'FamilyType_Married', 'NumUnits_Mobile home','NumUnits_Single attached','NumUnits_Single detached','OwnRent_Mortgage','OwnRent_Outright','OwnRent_Rented','HeatingFuel_Coal','HeatingFuel_Electricity','HeatingFuel_Gas', 'HeatingFuel_None','HeatingFuel_Oil','HeatingFuel_Other','HeatingFuel_Solar','HeatingFuel_Wood','Language_AsianPacific','Language_English','Language_Other','Language_OtherEuropean','Language_Spanish'])
# Load the trained model created with train_model.py
model = joblib.load('acs_ny_familyincome_classifier_model.pkl')

# Create a numpy array based on the model's feature importances
# We can use this feature for large data set, which one to keep and which oneto throw
importance = model.feature_importances_

# Sort the feature labels based on the feature importance rankings from the model
feauture_indexes_by_importance = importance.argsort()

# Print each feature label, from most important to least important (reverse order)
for index in feauture_indexes_by_importance:
    print("{} - {:.2f}%".format(feature_labels[index], (importance[index] * 100.0)))
    
    
